{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a631c425-20d4-44fa-88d3-22a26d578db0",
   "metadata": {},
   "source": [
    "LOVITON Pierre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2008e466-a363-4a71-84c5-9f39939032fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e04615e-ad27-41dd-851b-5913ca645561",
   "metadata": {},
   "source": [
    "**K-nearest neighbors** algorithm is a supervised learning algorithm. Easy to implement, it can be used to solve classification and regression problems. Here we use it as a classifier.\n",
    "\n",
    "\n",
    "**Advantages**:\n",
    "- Easy to implement.\n",
    "- We don't need to tune too many parameters\n",
    "- Classifier and Regression algorithm\n",
    "- Intuitive, easy to understand and help to take decisions\n",
    "\n",
    "**Disadvantages**\n",
    "- The algorithm becomes slower with many independent variables and observations.\n",
    "\n",
    "**Intuition**\n",
    "- Step 1: Select the number K of neighbors\n",
    "- Step 2: Calculate the distance (euclidean or manhattan for example) from the non classified point to the other points\n",
    "- Step 3: Take K nearest neighbors according to the calculated distance.\n",
    "- Step 4: Among thoses K neighbors, count the number of points belonging to each category.\n",
    "- Step 5: Assign the new point to the most present category among these K neighbors.\n",
    "- Step 6: Our model is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b687145e-8bb5-41f5-b8a4-927ff9213836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accuracy: 84.55%"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from river import datasets\n",
    "from river import evaluate\n",
    "from river import metrics\n",
    "from river import neighbors\n",
    "from river import preprocessing\n",
    "\n",
    "dataset = datasets.Phishing()\n",
    "\n",
    "model_knn = (\n",
    "    preprocessing.StandardScaler() |\n",
    "    neighbors.KNNClassifier(window_size=50)\n",
    ")\n",
    "\n",
    "evaluate.progressive_val_score(dataset, model_knn, metrics.Accuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "74019581-b41f-4eac-8c25-189b6bdee172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F1: 82.33%"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate.progressive_val_score(dataset, model_knn, metrics.F1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5a2af2b1-76fe-40c2-9030-0e010e3cb3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ROCAUC: 91.54%"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate.progressive_val_score(dataset, model_knn, metrics.ROCAUC())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81c78d3-cc82-473b-9a34-fa92cf6a5473",
   "metadata": {},
   "source": [
    "### With Manhattan distance instead of Euclidian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9b6840-7a75-4cb8-aeab-fac5c04e53d6",
   "metadata": {},
   "source": [
    "Manhattan distance (L1 norm) may be preferable to Euclidean distance (L2 norm) for the case of high dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "655f532c-c9e2-477d-af76-a6e7177c80ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accuracy: 86.87%"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "from river import utils\n",
    "model_knnman = (\n",
    "    preprocessing.StandardScaler() |\n",
    "    neighbors.KNNClassifier(\n",
    "        window_size=50,\n",
    "        distance_func=functools.partial(utils.math.minkowski_distance, p=1)\n",
    "    )\n",
    ")\n",
    "evaluate.progressive_val_score(dataset, model_knnman, metrics.Accuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "bac3093e-5036-4531-a0e8-4d26e36da741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F1: 85.21%"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate.progressive_val_score(dataset, model_knnman, metrics.F1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2fed1700-a399-43a7-9043-84b0dde90aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ROCAUC: 93.10%"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate.progressive_val_score(dataset, model_knnman, metrics.ROCAUC())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f7c057-4f3a-4f1a-917f-b8c6fd7bccba",
   "metadata": {},
   "source": [
    "# AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd1c91-e07b-4fbd-9711-c5fbb1968f41",
   "metadata": {},
   "source": [
    "**Boosting algorithms** are a set of the low accurate classifier to create a highly accurate classifier. Low accuracy classifier (or weak classifier) offers the accuracy better than the flipping of a coin. Highly accurate classifier (or strong classifier) offer a error rate close to 0. Boosting algorithm can track the model who failed the accurate prediction. They are less affected by overfitting problems.\n",
    "\n",
    "**AdaBoost** or Adaptativ Boosting is one of ensemble boosting classifier. \n",
    "It combines multiple classifiers to increase the accuracy of classifiers. \n",
    "AdaBoost is an iterative ensemble method.\n",
    "It builds a strong classifier by combining multiple poorly performing classifiers to get high accuracy strong classifier.\n",
    "The basic concept behinf AdaBoost is to set the weights of classifier and training the data sample at each iteration such that it ensures the accurate predictions of unusual observations. Any machine learning algorithm can be used as base classifier if it accepts weights on the training set. Adaboost should meet two conditions :\n",
    "- The classifier should be trained interactively on various weighed training examples.\n",
    "- At each iteration, it tries to provide an excellent fit for these examples by minimizing training error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "387c2f3b-d989-4c8d-975d-7ccbbd2c85e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import datasets\n",
    "from river import ensemble\n",
    "from river import evaluate\n",
    "from river import metrics\n",
    "from river import tree\n",
    "\n",
    "dataset = datasets.Phishing()\n",
    "\n",
    "# metric = metrics.LogLoss()\n",
    "\n",
    "model_ada = ensemble.AdaBoostClassifier(\n",
    "    model=(\n",
    "        tree.HoeffdingTreeClassifier(\n",
    "            split_criterion='gini',\n",
    "            delta=1e-5,\n",
    "            grace_period=2000\n",
    "        )\n",
    "    ),\n",
    "    n_models=5,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "253c384b-6412-47ac-bf16-c097cccbfe4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accuracy: 88.63%"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate.progressive_val_score(dataset, model_ada, metrics.Accuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cfcc0ec3-e2c4-493a-aaeb-4ef408a6cf9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F1: 87.56%"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate.progressive_val_score(dataset, model_ada, metrics.F1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "88ad5508-3d92-4c86-82ac-33f4c72badba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ROCAUC: 96.35%"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate.progressive_val_score(dataset, model_ada, metrics.ROCAUC())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91663dea-cfe1-4ba9-ab95-c8506644b53f",
   "metadata": {},
   "source": [
    "# BaggingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743c8cc7-9920-4240-9131-cad66c88a4e6",
   "metadata": {},
   "source": [
    "**Bagging**: when we generate multiple versiones of predictors and aggregate them to use in a model.\n",
    "The procedure of applying Bootstrap sampling on the training dataset and then Aggregating when estimating a numerical outcome and doing a plurality vote when predicting a class is called Bagging\n",
    "\n",
    "**The Bagging Algorithm**\n",
    "- Step 1: Generate Bootstrap Samples of same size (with replacement) are repeatedly taken from the training dataset, so that each record has an equal probability of being selected.\n",
    "- Step 2: A classification or estimation model is trained on each bootstrap sample drawn in Step 1, and a prediction is recorded for each sample.\n",
    "- Step 3: The bagging ensemble prediction is then defined to be the class with the most votes in Step 2 (for classification models) or the average of the predictions made in Step 2 (for estimation models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3b8e1fb8-d042-43ba-9708-fa131e27ecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import datasets\n",
    "from river import ensemble\n",
    "from river import evaluate\n",
    "from river import linear_model\n",
    "from river import metrics\n",
    "from river import optim\n",
    "from river import preprocessing\n",
    "\n",
    "dataset = datasets.Phishing()\n",
    "\n",
    "model_bagging = ensemble.BaggingClassifier(\n",
    "    model=(\n",
    "        preprocessing.StandardScaler() |\n",
    "        linear_model.LogisticRegression()\n",
    "    ),\n",
    "    n_models=3,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# metric = metrics.F1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "09e2f96a-ad30-46c8-b522-f640c4d8d39b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accuracy: 89.20%"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate.progressive_val_score(dataset, model_bagging, metrics.Accuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5a291ebe-e8e3-441e-85f9-34d4e8a57018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F1: 88.69%"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate.progressive_val_score(dataset, model_bagging, metrics.F1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "38173114-42b7-461f-a8ed-ef6c5411b8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ROCAUC: 95.93%"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate.progressive_val_score(dataset, model_ada, metrics.ROCAUC())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3e99f4-3458-4180-8d14-8968411a449c",
   "metadata": {},
   "source": [
    "# VotingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2349bd0-b861-40d7-b2c7-a4d605377db4",
   "metadata": {},
   "source": [
    "A **Voting Classifier** is a machine learning model that trains on an ensemble of numerous models and\n",
    "predicts an output (class) based on their highest probability of chosen class as the output. It simply\n",
    "aggregates the findings of each classifier passed into Voting Classifier and predicts the output class based\n",
    "on the highest majority of voting. The idea is instead of creating separate dedicated models and finding\n",
    "the accuracy for each of them, we create a single model which trains by these models and predicts output\n",
    "based on their combined majority of voting for each output class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8f3bee44-8332-4cb3-ac68-8f2801197963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import datasets\n",
    "from river import ensemble\n",
    "from river import evaluate\n",
    "from river import linear_model\n",
    "from river import metrics\n",
    "from river import naive_bayes\n",
    "from river import preprocessing\n",
    "from river import tree\n",
    "\n",
    "dataset = datasets.Phishing()\n",
    "\n",
    "model_voting = (\n",
    "    preprocessing.StandardScaler() |\n",
    "    ensemble.VotingClassifier([\n",
    "        linear_model.LogisticRegression(),\n",
    "        tree.HoeffdingTreeClassifier(),\n",
    "        naive_bayes.GaussianNB()\n",
    "    ])\n",
    ")\n",
    "\n",
    "# metric = metrics.F1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8e7985ba-923d-4485-a23b-9f1f62efb8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accuracy: 88.48%"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate.progressive_val_score(dataset, model_voting, metrics.Accuracy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4c65ee5b-10d8-4461-944e-022410cbe6a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F1: 88.91%"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate.progressive_val_score(dataset, model_voting, metrics.F1())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3714e2-5b10-468d-a779-9d857250c6be",
   "metadata": {},
   "source": [
    "# ADWINBaggingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9d827b-1875-4526-8e34-87ca1847586e",
   "metadata": {},
   "source": [
    "**ADWIN Bagging** is the online bagging method with the addition of the\n",
    "ADWIN algorithm as a change detector. If concept drift is detected, the worst member of the\n",
    "ensemble (based on the error estimation by ADWIN) is replaced by a new (empty) classifier.\n",
    "\n",
    "ADWIN is parameter- and assumption-free in the sense that it automatically detects and adapts to the current rate of change. Its only parameter is a confidence bound δ, indicating how confident we want to be in the algorithm’s output, inherent to all algorithms dealing with random processes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7f4db0a-5b02-4ee5-b629-c983e14e8c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from river import datasets\n",
    "from river import ensemble\n",
    "from river import evaluate\n",
    "from river import linear_model\n",
    "from river import metrics\n",
    "from river import optim\n",
    "from river import preprocessing\n",
    "\n",
    "dataset = datasets.Phishing()\n",
    "\n",
    "model_adwin = ensemble.ADWINBaggingClassifier(\n",
    "    model=(\n",
    "        preprocessing.StandardScaler() |\n",
    "        linear_model.LogisticRegression()\n",
    "    ),\n",
    "    n_models=3,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# metric = metrics.F1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b46c4ae-f7f0-4c01-a523-89b538769bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accuracy: 89.20%"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate.progressive_val_score(dataset, model_adwin, metrics.Accuracy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abd313d2-10af-4f33-9ead-dcb4c9a4faab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F1: 88.61%"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate.progressive_val_score(dataset, model_adwin, metrics.F1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff333d0-6647-4519-8263-fbe0e34f5c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ROCAUC: 96.01%"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate.progressive_val_score(dataset, model_adwin, metrics.ROCAUC())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
